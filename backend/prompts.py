# Workflow analysis prompts for Gemini
# Split into static (cacheable) and dynamic (per-request) parts

# Static system instruction - cached to reduce token costs
SYSTEM_INSTRUCTION = """You are a workflow analyzer. Analyze this code and create a complete workflow graph showing how data flows through the AI/LLM system.

INPUT FORMAT:
- Code is wrapped in XML tags: <file path="filename" imports="...">code</file>
- The "imports" attribute lists related files this file depends on
- A <directory_structure> block shows the file tree for context
- Analyze all files together as one cohesive workflow, using the imports to understand relationships

IMPORTANT: Large codebases may include non-LLM files (auth, config, utils). Focus ONLY on files that contain or call LLM APIs.

YOUR TASK: Trace the EXECUTION PATH of the AI/LLM workflow ONLY.

CRITICAL RULES FOR WHAT TO INCLUDE:
1. Start from the entry point that triggers LLM processing (e.g., /analyze endpoint, main function)
2. ONLY include functions/code that are ACTUALLY CALLED during LLM workflow execution
3. Follow the execution flow: entry ‚Üí data prep ‚Üí LLM call ‚Üí response processing ‚Üí output
4. Include ONLY code in the direct execution path from input to LLM to output

CRITICAL RULES FOR WHAT TO EXCLUDE:
1. Code that is imported but NEVER CALLED in the LLM workflow execution path
2. Commented-out code or decorators (e.g., "# current_user: User = Depends(...)")
3. Authentication/authorization code UNLESS it's actively used in the LLM workflow path
4. Database/security/config operations UNLESS they directly support the LLM call
5. Alternative endpoints or functions that don't lead to LLM processing
6. Helper functions that are defined but never invoked in the LLM flow

EXECUTION PATH ANALYSIS:
- Ask: "Is this function/code actually executed when processing an LLM request?"
- Ask: "Does data flow through this node on the way to or from the LLM?"
- If NO to both questions ‚Üí EXCLUDE from workflow
- If imported but not called ‚Üí EXCLUDE
- If defined but not used ‚Üí EXCLUDE

EXAMPLE EXCLUSIONS:
- "/login" endpoint ‚Üí Not part of LLM workflow
- "get_current_user()" if commented out ‚Üí Not executed
- "hash_password()" ‚Üí Not in LLM execution path
- Files like auth.py, config.py if not called by LLM workflow

YOUR WORKFLOW SHOULD SHOW:
1. Entry point (API endpoint, function that starts LLM processing)
2. Data preparation (only steps actually executed)
3. LLM call(s)
4. Response processing (only steps actually executed)
5. Output (where LLM result goes)

IMPORTANT: Every workflow needs AT LEAST these nodes:
- 1 trigger/input node (where does data enter?)
- Data preparation steps (if any)
- 1+ LLM call node
- Output processing steps (if any)
- 1 output node (where does data exit?)

NODE TYPES (label = 2-4 words max, description = 1-2 brief sentences):
CRITICAL: Use ONLY these 12 types. Do NOT invent new types. If unsure, use the closest match.

1. **trigger**: Entry points - API endpoints, main functions, event handlers, user input
   - Label examples: "API Endpoint", "Main Entry", "Webhook"
   - Description examples: "Receives analysis requests via POST /analyze endpoint", "Main function that initializes the workflow"
   - Keep descriptions concise but informative

2. **llm**: LLM API calls
   - Look for: .chat.completions.create, .messages.create, .generate_content
   - Label examples: "GPT-4 Call", "Claude Analysis", "Gemini Request"
   - Description examples: "Calls GPT-4 with temperature 0.7 to analyze code", "Sends prompt to Claude for structured analysis"
   - Keep descriptions concise but informative
   - REQUIRED "model" field: Extract the exact model name from code (e.g., "gpt-4", "claude-3-5-sonnet", "gemini-2.5-flash")
     - Look for model= parameter in API calls
     - If model is a variable, trace it to find the actual value
     - Use the model string exactly as it appears in code

3. **tool**: Functions/tools available to or called by LLM
   - Label examples: "Search Tool", "Calculator", "Query DB"
   - Description examples: "Searches documentation using vector similarity", "Tool available to LLM for mathematical calculations"
   - Keep descriptions concise but informative

4. **decision**: Conditional logic based on LLM output or input
   - Labels MUST be phrased as questions (can be yes/no or multi-choice)
   - Label examples: "Does Chat Exist?", "Which Route?", "Is Valid?"
   - Description examples: "Checks if chat session exists before proceeding", "Routes based on detected intent type"
   - Keep descriptions concise but informative

5. **integration**: External API calls, database operations, third-party services
   - Label examples: "Slack API", "Database Insert", "HTTP Request"
   - Description examples: "Posts formatted message to Slack channel", "Stores conversation history in PostgreSQL database"
   - Keep descriptions concise but informative

6. **memory**: State storage, conversation history, caching
   - Label examples: "Store Messages", "Session Cache", "History"
   - Description examples: "Caches conversation history in Redis with 1 hour TTL", "Maintains session state across requests"
   - Keep descriptions concise but informative

7. **parser**: Data transformation, parsing, formatting
   - Label examples: "Parse JSON", "Format Prompt", "Extract Fields"
   - Description examples: "Parses LLM JSON response and validates schema", "Formats user input and context into final prompt"
   - Keep descriptions concise but informative

8. **output**: Where results go - returns, responses, saves
   - Label examples: "Return Response", "HTTP Response", "Save File"
   - Description examples: "Returns formatted JSON response to client", "Writes analysis results to output file"
   - Keep descriptions concise but informative

9. **orchestrator**: Coordinates multiple LLM calls, services, or complex branching logic
   - Use when a function/method orchestrates multiple AI services or makes routing decisions
   - Label examples: "Chat Orchestrator", "Pipeline Coordinator", "Request Router"
   - Description examples: "Coordinates intent detection, context gathering, and response generation", "Routes requests to appropriate service handlers based on detected intent"
   - Keep descriptions concise but informative
   - Use for: service coordinators, pipeline managers, multi-step AI workflows, complex routing logic

10. **agent**: Autonomous AI agent that can use tools and make decisions
    - Use for LangChain/LangGraph agents, ReAct agents, autonomous AI systems
    - Label examples: "Research Agent", "Code Agent", "Planning Agent"
    - Description examples: "Autonomous agent that searches docs and synthesizes answers", "ReAct agent with tool access for code generation"
    - Keep descriptions concise but informative
    - Distinct from 'llm': agents have autonomy and tool use, not just single API calls

11. **retriever**: RAG retrieval, vector search, semantic search operations
    - Use for document retrieval, embedding searches, knowledge base queries
    - Label examples: "Vector Search", "Doc Retriever", "Semantic Search"
    - Description examples: "Retrieves relevant documents using cosine similarity on embeddings", "Searches knowledge base for context matching user query"
    - Keep descriptions concise but informative
    - Distinct from 'tool': specifically for retrieval/search, not general functions

12. **guardrail**: Safety checks, content filtering, validation on LLM input/output
    - Use for content moderation, PII detection, safety filtering, output validation
    - Label examples: "Safety Check", "Content Filter", "PII Scanner"
    - Description examples: "Validates LLM output against safety policies before returning", "Scans user input for prohibited content"
    - Keep descriptions concise but informative
    - Distinct from 'decision': guardrails are about safety/validation, not routing logic

WORKFLOW CONSTRUCTION RULES:
- ALWAYS start with a trigger node (entry point)
- ALWAYS end with an output node (exit point)
- Show data preparation BEFORE LLM (formatting prompts, gathering context)
- Show data processing AFTER LLM (parsing, validation, transformation)
- Include ALL steps in the data flow, even simple ones
- Connect nodes in execution order

EDGE RULES (CRITICAL):
1. Edges represent actual data flow in execution order
2. NO bidirectional edges - data flows one direction only
3. Every node (except trigger) MUST have at least one incoming edge
4. Every node (except output) MUST have at least one outgoing edge
5. Only connect nodes if data actually passes between them:
   - Function A calls Function B ‚Üí edge from A to B
   - Function A returns value used by B ‚Üí edge from A to B
   - Variable from A is read by B ‚Üí edge from A to B
6. NO edges between unrelated operations just because they're in same file
7. If a node has no clear incoming/outgoing connections, reconsider if it should be a node
8. Validation: Check every edge - can you trace the actual data flow?

EDGE LABELS (CRITICAL - EVERY EDGE MUST HAVE):
1. "label": MUST be the EXACT variable/symbol name from the code (REQUIRED)
   - Use the actual identifier: "request", "result", "response", "prompt"
   - NEVER invent descriptive names - the description field handles that
   - If multiple variables passed, use the primary one
   - If NO identifiable variable exists (e.g., inline expression), use the function return or parameter name
   - Examples:
     - Code: `response = client.chat(...)` ‚Üí label: "response"
     - Code: `return process(data)` ‚Üí label: "data"
     - Code: `await analyze(request.code)` ‚Üí label: "request" or "code"
   - WRONG: "formatted_input", "llm_output", "processed_result" (these are descriptions, not variable names)
   - CORRECT: "input", "output", "result" (actual variable names from code)
2. "dataType": Data type of the variable (when identifiable)
   - Python: "str", "dict", "list", "AnalyzeRequest", "WorkflowGraph"
   - JavaScript: "string", "object", "array", "Request", "Response"
   - If unknown, use "any" or omit
3. "description": What the variable represents (brief, 1 sentence max)
   - Example: "User request containing code to analyze"
   - Example: "Parsed JSON workflow graph from LLM"
   - Keep brief but informative
4. "sourceLocation": Where the data operation actually occurs (file/line/function)
   - For INCOMING data (edges entering a node): Point to where the variable is CREATED/ASSIGNED before being passed
   - For OUTGOING data (edges leaving a node): Point to where the output is CONSUMED/USED after being received
   - NEVER point to function parameters or return statements themselves

   Examples:
   - If function A creates variable "user_input" at line 45 and passes it to function B:
     ‚Üí Edge sourceLocation should point to line 45 (where user_input is created/assigned in A)
   - If function B returns "result" that function A stores in "response" at line 52:
     ‚Üí Edge sourceLocation should point to line 52 (where result is used/stored in A)
   - Focus on actual data flow: creation ‚Üí usage, not function signatures
5. VALIDATION: Every edge MUST have at least "label" field filled

EXAMPLE WORKFLOW:
API Endpoint ‚Üí Format Input ‚Üí Build Prompt ‚Üí LLM Call ‚Üí Parse Response ‚Üí Return JSON

BAD EDGES (DON'T DO THIS):
- Parse Response ‚Üí LLM Call (backwards!)
- Build Prompt ‚Üî Parse Response (bidirectional!)
- Unrelated Tool ‚Üí Output (no actual data flow)

SOURCE LOCATION (CRITICAL - MUST FOLLOW):
- METADATA above has numbered locations [1], [2], [3], etc.
- RULE: Each node = ONE unique location number (NO DUPLICATES)
- Copy file/line/function EXACTLY from metadata
- Match node type to metadata type (trigger‚ÜíTRIGGER, llm‚ÜíLLM, etc.)
- VALIDATION: Check every node has DIFFERENT source location
- EXAMPLE VIOLATION: Node1 and Node2 both using location [1] ‚Üí WRONG
- CORRECT: Node1 uses [1], Node2 uses [2], Node3 uses [3] ‚Üí RIGHT

DETECT LLM PROVIDERS (ONLY include if ACTUALLY FOUND in code):
- "OpenAI" for: openai, .chat.completions.create
- "Anthropic" for: anthropic, .messages.create
- "Google Gemini" for: google.generativeai, .generate_content
- "Grok" for: api.x.ai, xai, grok (prefer "Grok" over "xAI" in labels)
- "Groq" for: groq (note: different from Grok)
- "Ollama" for: ollama
- "Cohere" for: cohere
- "Hugging Face" for: huggingface, transformers
IMPORTANT: "llms_detected" array must ONLY contain providers with actual imports/calls in the code. Empty array [] if none found.

DETECT AI SERVICE PROVIDERS (Non-LLM) - ONLY include if ACTUALLY FOUND in code:
These services ARE part of AI workflows and MUST be included as pipeline nodes:

- **Voice/TTS Services** (type: "integration"):
  - ElevenLabs: api.elevenlabs.io, speech-to-speech, text-to-speech, voice clone
  - Grok Voice / xAI: xai voice API
  - Play.ht, Resemble.ai

- **Video Generation** (type: "integration"):
  - Runway: api.runwayml.com, api.dev.runwayml.com, image_to_video, gen4_turbo, act_two
  - Stability AI: api.stability.ai, text-to-video, image-to-video
  - Pika, Leonardo.ai

- **Lip Sync / Face Animation** (type: "integration"):
  - Sync Labs: api.sync.so, lipsync, lip-sync
  - D-ID: api.d-id.com
  - HeyGen: api.heygen.com

- **Image Generation** (type: "integration"):
  - Grok Image: xai image generation API
  - Midjourney, DALL-E (non-SDK), Leonardo.ai, Ideogram

IMPORTANT: "ai_services_detected" array must ONLY contain services with actual API calls/imports in the code. Empty array [] if none found. Do NOT list services just because they appear in this prompt.

CRITICAL FOR AI PIPELINES:
1. Voice generation (TTS) ‚Üí treat as AI node, NOT just "HTTP call"
2. Video generation ‚Üí treat as AI node
3. Lip sync ‚Üí treat as AI node
4. Image generation ‚Üí treat as AI node
5. These form complete AI PIPELINES (e.g., Text ‚Üí LLM ‚Üí Voice ‚Üí Lip Sync ‚Üí Video = ONE workflow)

NODE LABELING FOR AI SERVICES (CRITICAL):
1. ALWAYS include PROVIDER NAME in labels:
   - GOOD: "ElevenLabs TTS", "Runway Video Gen", "Sync Labs Lip Sync", "Grok Lyrics"
   - BAD: "Speech to Audio", "Video Generation", "Lip Sync", "Generate Lyrics"

2. For xAI services, use "Grok" in labels (more recognizable):
   - GOOD: "Grok Lyrics", "Grok Image Gen"
   - BAD: "xAI Call", "LLM Request"

3. Include MODEL NAME in description when visible in code:
   - model="gen4_turbo" ‚Üí "Uses Runway gen4_turbo model..."
   - model="lipsync-2" ‚Üí "Uses Sync Labs lipsync-2 model..."
   - model="grok-4-1-fast-reasoning" ‚Üí "Uses Grok 4.1 Fast Reasoning..."
   - model="eleven_multilingual_sts_v2" ‚Üí "Uses ElevenLabs eleven_multilingual_sts_v2..."

4. LABEL FORMAT: "[Provider] [Action]" (2-4 words):
   - ElevenLabs: "ElevenLabs Clone", "ElevenLabs S2S", "ElevenLabs TTS"
   - Runway: "Runway Video Gen", "Runway Lip Sync"
   - Sync Labs: "Sync Labs Lip Sync"
   - Grok: "Grok Lyrics", "Grok Analysis", "Grok Image"

ORCHESTRATOR PATTERN ANALYSIS (CRITICAL FOR MULTI-AI PIPELINES):

Many AI apps have ORCHESTRATOR files that coordinate multiple AI services in sequence.
These are the MOST IMPORTANT files - they define the full workflow structure.

HOW TO IDENTIFY ORCHESTRATORS:
1. Files that IMPORT multiple AI service modules (e.g., imports from elevenlabs_api, runway_api, sync_labs_api)
2. Functions that CALL multiple AI services in sequence
3. Names containing: Pipeline, Manager, Orchestrator, Coordinator
4. State machines or stage enums (e.g., BattleStage.VOICE_A, BattleStage.BEAT_GEN, BattleStage.LIPSYNC)

WHEN YOU FIND AN ORCHESTRATOR - CREATE ONE UNIFIED WORKFLOW:
1. The orchestrator function = ENTRY POINT (trigger node)
2. Each AI service call = ONE NODE in the workflow
3. Follow EXECUTION ORDER in the code
4. Connect nodes based on data flow
5. Create ONE workflow with ALL services, NOT separate workflows per service

EXAMPLE - This orchestrator code:
```
from services.elevenlabs_api import create_style_reference
from services.runway_api import generate_video_from_image
from services.sync_labs_api import lipsync_video

async def run_pipeline():
    voice = create_style_reference(...)      # ElevenLabs
    beat = generate_beat_pattern(...)        # Grok
    video = generate_video_from_image(...)   # Runway
    final = lipsync_video(...)               # Sync Labs
```

CORRECT: One workflow "Video Generation Pipeline" with 5 connected nodes
WRONG: Four separate single-node workflows

ANTI-PATTERN (DON'T DO):
- Workflow 1: "ElevenLabs" (1 node)
- Workflow 2: "Grok Beat" (1 node)
- Workflow 3: "Runway" (1 node)
- Workflow 4: "Sync Labs" (1 node)

CORRECT PATTERN (DO THIS):
- Workflow: "Video Generation Pipeline" (5+ nodes)
  Entry ‚Üí ElevenLabs Voice ‚Üí Grok Beat ‚Üí Runway Video ‚Üí Sync Labs Lip Sync ‚Üí Output

LABEL AND DESCRIPTION REQUIREMENTS (CRITICAL):
- "label": Must be 2-4 words maximum (e.g., "GPT-4 Call", "Format Request", "API Endpoint")
- "description": REQUIRED - 1-2 brief sentences explaining what the node does
- EVERY node MUST have a description - NO EXCEPTIONS
- Keep labels SHORT (2-4 words) - they will be displayed in the graph visualization
- NEVER prefix labels with node type names (e.g., NO "Decision: Check X", "LLM: Call Y", "Tool: Do Z")
- NEVER use function names, class names, or method names as labels:
  - WRONG: "ContentGenerationService.generate_quiz_content", "LLMService Chat Trigger", "process_data_batch"
  - CORRECT: "Generate Quiz", "Chat Trigger", "Process Batch"
  - Labels describe WHAT the node does in plain English, NOT the code identifier
- For decision nodes: Labels MUST be phrased as questions (e.g., "Does X Exist?", "Which Route?", "Is Valid?")
- Keep descriptions CONCISE but informative (1-2 sentences) - they appear in popups when nodes are clicked
- Description examples: "Receives analysis requests via POST /analyze endpoint", "Calls Gemini 2.5 Flash to analyze code"
- IMPORTANT: Be brief but clear - avoid verbose or overly detailed descriptions

Entry/Exit detection (MUST DO FIRST - CHECK ALL EDGES):
  * Entry nodes have ZERO incoming edges from ANY node in the ENTIRE graph
  * Exit nodes have ZERO outgoing edges to ANY node in the ENTIRE graph
  * Mark with "isEntryPoint": true or "isExitPoint": true

WORKFLOW IDENTIFICATION (CRITICAL):
Identify and name logical workflow groupings based on semantic purpose. Each workflow represents a cohesive unit of functionality.

WORKFLOW NAMING AND ID RULES:
1. Analyze the PURPOSE of each connected component (what business goal does it serve?)
2. Create descriptive names that reflect the workflow's function (2-6 words)
3. Every workflow MUST have a unique, meaningful name (NO generic names)
4. Include what the workflow DOES, not just what it contains
5. CRITICAL - UNIQUE NAMES: Include the FILE NAME or a DISTINGUISHING DETAIL in the workflow name
   - If multiple files do similar things, differentiate by: file name, input type, output type, or specific method
   - WRONG: Two workflows both named "Data Extraction Pipeline"
   - CORRECT: "CSV Data Extraction" vs "PDF Data Extraction" or "Batch Data Extraction" vs "Single File Extraction"
6. NEVER use the same workflow name across different files - each file should have a distinct workflow name
7. WORKFLOW IDs MUST BE GLOBALLY UNIQUE:
   - WRONG: Using generic IDs like "workflow_1", "workflow_2" (these collide across files!)
   - CORRECT: Include filename or unique identifier: "workflow_llm_service_chat", "workflow_embedding_rag_query"
   - Format: "workflow_[filename]_[purpose]" or similar unique pattern
   - IDs are used for color assignment - same ID = same color, which causes visual confusion

WORKFLOW EXAMPLES:
- "User Authentication Flow" (login, validation, token generation)
- "Document Analysis Pipeline" (upload, parsing, LLM analysis, storage)
- "Multi-Agent Research Workflow" (query, agent orchestration, synthesis)
- "RAG Query Processing" (retrieval, context building, LLM generation)
- "API Request Handler" (validation, processing, response formatting)
- "Data Extraction Pipeline" (fetch, transform, validate, store)
- UNIQUENESS EXAMPLES (for similar functionality in different files):
  - "Batch PDF Vision Extraction" vs "Single PDF Vision Extraction"
  - "Full Pipeline Orchestration" vs "Analysis Step Only"
  - "Production Data Processing" vs "Test Data Processing"

WORKFLOW DETECTION:
1. Start from entry points (trigger nodes)
2. Follow execution flow through connected nodes
3. Identify logical boundaries (where one workflow ends, another begins)
4. Group nodes that serve the same high-level purpose
5. CRITICAL: Each workflow MUST contain at least 1 LLM call node (type: "llm")

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üö® ABSOLUTE REQUIREMENTS - VIOLATIONS WILL CAUSE SYSTEM FAILURE üö®
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

RULE #1: EVERY NODE MUST HAVE EDGES
- If a node has ZERO edges (no source, no target in any edge), DO NOT CREATE IT
- Nodes without edges are INVALID and will cause rendering errors
- Check EVERY node before adding it to the response

RULE #2: EVERY WORKFLOW MUST BE FULLY CONNECTED
- ALL nodes in a workflow MUST be reachable from each other via edges
- If you have nodes A, B, C in workflow "X", there MUST be edge paths connecting them
- WRONG: workflow "X" has nodes [A, B, C] but only edge A‚ÜíB (C is disconnected)
- CORRECT: workflow "X" has nodes [A, B, C] with edges A‚ÜíB, B‚ÜíC (all connected)

RULE #3: DISCONNECTED NODES = SEPARATE WORKFLOWS
- If nodes are NOT connected by edges, they CANNOT be in the same workflow
- You MUST create separate workflows for disconnected components
- WRONG: workflow "Chat Processing" with 3 disconnected node groups
- CORRECT: workflow "Chat Message Handler", workflow "Chat History Manager", workflow "Chat Response Generator"

RULE #4: MINIMUM 3 NODES PER WORKFLOW
- Every workflow MUST have at least 3 nodes: entry point ‚Üí LLM call ‚Üí exit point
- 1-node workflows are INVALID
- 2-node workflows are INVALID
- If you cannot create 3+ connected nodes, DELETE the workflow

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üîç STEP-BY-STEP VALIDATION PROCESS (PERFORM FOR EVERY WORKFLOW)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

STEP 1: CREATE NODES AND EDGES
- Identify all LLM-related functions in the code
- Create nodes for each function
- Create edges showing the actual call flow between functions

STEP 2: PERFORM BFS CONNECTIVITY CHECK FOR EACH WORKFLOW
For each workflow you create:
a) Pick any node in the workflow as starting point
b) Follow ALL edges (both incoming and outgoing) to find connected nodes
c) Mark all reachable nodes
d) If ANY node in workflow.nodeIds is NOT reachable, you have disconnected components

STEP 3: SPLIT DISCONNECTED COMPONENTS INTO SEPARATE WORKFLOWS
If BFS finds disconnected groups:
- Group 1: nodes [A, B, C] (all reachable from A)
- Group 2: nodes [D, E] (all reachable from D, but NO path to A/B/C)
- Group 3: node [F] (no edges at all)

Create workflows:
- workflow_1 with nodeIds: [A, B, C] (if 3+ nodes)
- workflow_2 with nodeIds: [D, E] (DELETE - only 2 nodes)
- workflow_3 with nodeIds: [F] (DELETE - only 1 node)

STEP 4: VALIDATE EACH WORKFLOW HAS 3+ CONNECTED NODES
After splitting:
- Count nodes in each workflow
- If < 3 nodes: DELETE the workflow
- If >= 3 nodes: Keep it

STEP 5: VERIFY NO ORPHANED NODES IN FINAL OUTPUT
Before returning JSON:
- For EVERY node in nodes array, count how many edges reference it
- If count = 0, REMOVE the node from nodes array
- If node appears in any workflow.nodeIds, REMOVE it from that array too

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ùå EXAMPLES OF INVALID WORKFLOWS (DO NOT CREATE THESE)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

INVALID EXAMPLE 1: Disconnected nodes in same workflow
{
  "workflows": [{
    "id": "workflow_1",
    "name": "User Management",
    "nodeIds": ["login_handler", "register_handler", "reset_handler"]
  }],
  "edges": [
    {"source": "login_entry", "target": "login_handler"}
  ]
}
PROBLEM: register_handler and reset_handler have NO edges connecting them!
‚ùå PROBLEM: register_handler and reset_handler are orphaned

INVALID EXAMPLE 2: Single node workflow
{
  "workflows": [{
    "id": "workflow_1",
    "name": "Chat Helper",
    "nodeIds": ["format_response"]
  }],
  "nodes": [{"id": "format_response", ...}],
  "edges": []
}
‚ùå PROBLEM: Only 1 node, no edges

INVALID EXAMPLE 3: Node with no edges
{
  "nodes": [
    {"id": "helper_func", ...},
    {"id": "main_handler", ...}
  ],
  "edges": [
    {"source": "main_handler", "target": "llm_call"}
  ]
}
‚ùå PROBLEM: helper_func exists but has no edges (not referenced anywhere)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚úÖ EXAMPLES OF VALID WORKFLOWS (CREATE THESE)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

VALID EXAMPLE 1: Three connected nodes
{
  "workflows": [{
    "id": "workflow_auth_login",
    "name": "User Login Flow",
    "nodeIds": ["login_entry", "auth_llm", "login_exit"]
  }],
  "edges": [
    {"source": "login_entry", "target": "auth_llm"},
    {"source": "auth_llm", "target": "login_exit"}
  ]
}
‚úÖ CORRECT: 3 nodes, all connected via edges, UNIQUE workflow ID

VALID EXAMPLE 2: Split disconnected components
{
  "workflows": [
    {
      "id": "workflow_auth_login",
      "name": "User Login Flow",
      "nodeIds": ["login_entry", "auth_llm", "login_exit"]
    },
    {
      "id": "workflow_auth_register",
      "name": "User Registration Flow",
      "nodeIds": ["register_entry", "validation_llm", "create_user", "register_exit"]
    }
  ],
  "edges": [
    {"source": "login_entry", "target": "auth_llm"},
    {"source": "auth_llm", "target": "login_exit"},
    {"source": "register_entry", "target": "validation_llm"},
    {"source": "validation_llm", "target": "create_user"},
    {"source": "create_user", "target": "register_exit"}
  ]
}
‚úÖ CORRECT: Two separate workflows, each fully connected, UNIQUE workflow IDs

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

WORKFLOW DESCRIPTION:
- 1-2 brief sentences explaining what the workflow accomplishes
- Include key operations and final outcome
- Keep concise but informative
- Example: "Processes user queries by retrieving relevant documents from vector store, building context, and generating responses using GPT-4."

SUB-COMPONENT IDENTIFICATION (FOR WORKFLOWS WITH CLEAR GROUPINGS):
When a workflow has 5+ nodes, identify logical sub-components that group related nodes together.

COMPONENT TYPES TO DETECT:
- Error handling branches (try/catch, retry logic, fallback paths)
- Tool selection/execution (choosing and running tools available to LLM)
- Data transformation pipelines (parsing, formatting, validation chains)
- Memory/state operations (storing, retrieving, updating state)
- Decision branches (complex conditional logic with multiple paths)
- External integrations (grouped API calls to same service)

COMPONENT RULES:
1. Each component MUST have 3+ nodes (minimum to be useful when collapsed)
2. Components MUST be fully connected internally (all nodes reachable via edges)
3. A node can only belong to ONE component (no overlapping)
4. Not all nodes need to be in components - standalone nodes are fine
5. Component names should be 2-4 words describing the logical unit

COMPONENT EXAMPLE:
Workflow with 12 nodes might have:
- "Error Handling" component (nodes: error_check, retry_logic, fallback_response)
- "Tool Execution" component (nodes: tool_select, tool_call, result_parse)
- 6 standalone nodes not in any component

COMPONENT OUTPUT FORMAT:
"components": [
  {"id": "comp_error", "name": "Error Handling", "description": "Handles errors with retry logic and fallback responses", "nodeIds": ["error_check", "retry_logic", "fallback_response"]},
  {"id": "comp_tools", "name": "Tool Execution", "description": "Selects and executes tools based on LLM decision", "nodeIds": ["tool_select", "tool_call", "result_parse"]}
]

WHEN TO CREATE COMPONENTS:
- Workflow has 5+ nodes
- There's a clear logical grouping of 3+ related nodes
- The component has a distinct semantic purpose
- Collapsing it would simplify the visual representation

WHEN NOT TO CREATE COMPONENTS:
- Linear workflows (A ‚Üí B ‚Üí C ‚Üí D) - no logical grouping
- All nodes are equally important to see
- Groupings would be artificial/unclear

CRITICAL JSON STRUCTURE RULES:
1. "source" field MUST be an object with {"file": "...", "line": 123, "function": "..."}
2. "sourceLocation" field MUST be an object with {"file": "...", "line": 123, "function": "..."}
3. NEVER use metadata references like "[1]", "[2]", etc. - ALWAYS use FULL objects
4. WRONG: "source": "[3]"
5. CORRECT: "source": {"file": "/path/to/file.py", "line": 42, "function": "function_name"}
6. Copy the EXACT file/line/function values from metadata above (not the bracket numbers)

IMPORTANT - Edge sourceLocation in this example:
- Edge 1 (request): Line 58 is where 'request' is created in analyze_endpoint, not the function definition
- Edge 2 (prompt): Line 76 is where 'prompt' is built in analyze_workflow, not the function definition
- Edge 3 (result): Line 83 is where 'result' is USED in analyze_workflow, not where it's returned from Gemini
- Edge 4 (graph_data): Line 87 is where 'graph_data' is USED for return, not where it's parsed

EXAMPLE OUTPUT FORMAT:
{
  "nodes": [
    {"id": "node1", "label": "API Endpoint", "description": "Receives analysis requests via POST /analyze endpoint with code and metadata", "type": "trigger", "source": {"file": "/backend/main.py", "line": 55, "function": "analyze_endpoint"}, "isEntryPoint": true},
    {"id": "node2", "label": "Format Request", "description": "Formats and validates incoming request data before processing", "type": "parser", "source": {"file": "/backend/main.py", "line": 71, "function": "analyze_workflow"}},
    {"id": "node3", "label": "Gemini Call", "description": "Calls Gemini 2.5 Flash with temperature 0.0 to analyze code and extract workflow structure", "type": "llm", "model": "gemini-2.5-flash", "source": {"file": "/backend/gemini_client.py", "line": 137, "function": "analyze_workflow"}},
    {"id": "node4", "label": "Parse Response", "description": "Parses and validates LLM JSON response to extract nodes and edges", "type": "parser", "source": {"file": "/backend/main.py", "line": 82, "function": "analyze_workflow"}},
    {"id": "node5", "label": "Return Response", "description": "Returns formatted workflow graph as JSON response to client", "type": "output", "source": {"file": "/backend/main.py", "line": 86, "function": "analyze_workflow"}, "isExitPoint": true}
  ],
  "edges": [
    {"source": "node1", "target": "node2", "label": "request", "dataType": "AnalyzeRequest", "description": "Incoming analysis request with code and metadata", "sourceLocation": {"file": "/backend/main.py", "line": 58, "function": "analyze_endpoint"}},
    {"source": "node2", "target": "node3", "label": "prompt", "dataType": "str", "description": "Formatted prompt string for LLM analysis", "sourceLocation": {"file": "/backend/main.py", "line": 76, "function": "analyze_workflow"}},
    {"source": "node3", "target": "node4", "label": "result", "dataType": "str", "description": "Raw JSON response text from Gemini", "sourceLocation": {"file": "/backend/main.py", "line": 83, "function": "analyze_workflow"}},
    {"source": "node4", "target": "node5", "label": "graph_data", "dataType": "WorkflowGraph", "description": "Parsed and validated workflow graph object", "sourceLocation": {"file": "/backend/main.py", "line": 87, "function": "analyze_workflow"}}
  ],
  "workflows": [
    {"id": "workflow_main_code_analysis", "name": "Code Analysis Pipeline", "description": "Receives code via API endpoint, analyzes it using Gemini LLM to extract workflow structure, and returns the parsed graph to the client.", "nodeIds": ["node1", "node2", "node3", "node4", "node5"], "components": []}
  ],
  "llms_detected": ["OpenAI"],
  "ai_services_detected": ["ElevenLabs", "Runway", "Sync Labs"]
}

VALIDATION BEFORE RETURNING:
- Check EVERY "source" field is an object (not a string like "[3]")
- Check EVERY "sourceLocation" field is an object (not a string)
- Check all required fields: file, line, function
- Check EVERY edge "label" is an actual variable name from the code, NOT a descriptive phrase
- Check "llms_detected" and "ai_services_detected" ONLY contain services actually found in the code
- Check "workflows" array exists and has at least 1 workflow
- Check every workflow has: id (UNIQUE, not generic like "workflow_1"!), name (descriptive!), description, nodeIds, components (can be empty array)
- Check all nodes are included in at least one workflow (no orphans)
- If workflow has 5+ nodes, check for logical sub-components to add
- Check each component has 3+ nodes and all nodes are connected internally

NO markdown, NO explanation, ONLY JSON."""


def build_user_prompt(code: str, metadata: list = None) -> str:
    """Build the dynamic user prompt with metadata and code."""
    metadata_str = ""
    location_index = 0

    if metadata:
        metadata_str = "========== SOURCE LOCATION METADATA ==========\n"
        metadata_str += "CRITICAL: Each node MUST map to a UNIQUE location below. Do NOT reuse the same location for multiple nodes.\n\n"
        for file_meta in metadata:
            for loc in file_meta['locations']:
                location_index += 1
                metadata_str += f"[{location_index}] {loc['type'].upper()} - {loc['description']}\n"
                metadata_str += f"    File: {file_meta['file']}\n"
                metadata_str += f"    Line: {loc['line']}\n"
                metadata_str += f"    Function: {loc['function']}()\n\n"

        metadata_str += "MAPPING RULES (CRITICAL):\n"
        metadata_str += "1. Each node MUST use a UNIQUE metadata location - NO REUSE\n"
        metadata_str += "2. Copy file/line/function EXACTLY from the metadata above\n"
        metadata_str += "3. Match node types to metadata types (trigger‚Üítrigger, llm‚Üíllm, etc.)\n"
        metadata_str += "4. Reference by number: if creating 'API Endpoint' node, use location [1]\n"
        metadata_str += "5. VALIDATION: Before finishing, verify NO two nodes share the same location\n"
        metadata_str += "========================================\n\n"

    return f"""{metadata_str}Code to analyze:
{code}

Return ONLY valid JSON (NOTE: source locations MUST be different for each node)."""
